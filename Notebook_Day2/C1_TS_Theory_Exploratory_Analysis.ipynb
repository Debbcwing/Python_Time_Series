{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ff063b-385b-4894-a9b8-53ce4365bd9d",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30026fa9-017d-490c-a08b-0b3ae2376adc",
   "metadata": {},
   "source": [
    "# Time Series Theory in Python - Part 1: Exploratory Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae061fb-5200-490e-bbf3-6b7bf8d5ea3f",
   "metadata": {},
   "source": [
    "This notebook focuses on introducing basic concepts of time series analysis for univariate time series and then extending to some specific analysis for multivariate time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae09e8-8315-47fe-be70-8a3399b6f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PythonTsa statsmodels openpyxl seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea949e1b-f291-4f39-bb9b-8e1b31fc17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PythonTsa.datadir import getdtapath # We use the Python package PythonTsa to load datasets coming with this package.\n",
    "dtapath=getdtapath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a8401-c49e-416d-98a6-4d973349418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400c68f-158e-4f11-96ed-41e0bc442217",
   "metadata": {},
   "source": [
    "## 1. Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ad60c-0081-48dd-b5da-9facd92520ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic time series data\n",
    "time = pd.date_range(start=\"2020-01-01\", periods=100)\n",
    "trend = np.arange(100)  # Linear trend as a NumPy array\n",
    "seasonal = [10 * np.sin(i / 5) for i in range(100)]  # Seasonal component\n",
    "noise = np.random.normal(0, 2, size=100)  # Random noise\n",
    "\n",
    "# Combine trend, seasonal, and noise components into one array\n",
    "data = trend + seasonal + noise\n",
    "\n",
    "# Create DataFrame\n",
    "series = pd.Series(data, index=time)\n",
    "\n",
    "# Plot the time series\n",
    "series.plot()\n",
    "plt.title(\"Synthetic time series with trend and seasonality\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a889d-59ff-4176-b802-845222ec0fcc",
   "metadata": {},
   "source": [
    "## 2. Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a6d44-b541-4944-b6b4-0c54a4b5557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pandas.plotting import lag_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee369900-1f08-4cc8-bcc7-2eb9cb17a5e2",
   "metadata": {},
   "source": [
    "### Autocorrelation Function (ACF):\n",
    "\n",
    "The ACF Plot enables the evaluation of the correlation between an observation and its previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08801172-2327-4f35-adbb-54f17edbce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(series, lags=30, alpha=0.05) # alpha=0.05 is the default)\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e2053-a371-48d1-b153-5043356e09c6",
   "metadata": {},
   "source": [
    "### Partial Autocorrelation Function (PACF):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607f94eb-ae70-496b-ac99-485cdbc1c573",
   "metadata": {},
   "source": [
    "The PACF Plot helps to understand the direct relationship between an observation and its own lagged values, excluding the effects of other lags. The PACF clearly measures the correlation between $X_t$ and $X_{t-k}$ that is not explained by $\\{ X_{t-k+1}, \\dots, X_{t-1} \\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05ebfb-3e94-47ce-9f26-5e271e51df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(series, lags=30)\n",
    "plt.title(\"PACF of the Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6eba8-3b04-41b4-9abe-1ab8a6283c66",
   "metadata": {},
   "source": [
    "### Lag Plot:\n",
    "\n",
    "The Lag Plot offers a scatter plot that directly compares the values of the time series to its lagged versions, helping to identify any linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de70071-c668-446f-bacd-de966d72337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_plot(series)\n",
    "plt.title('Lag Plot of the Time Series')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad473cb6-5f4b-4520-ae78-a60739727abe",
   "metadata": {},
   "source": [
    "### **Example 1: Chinese Quarterly GDP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24bc5fc-f325-42d2-bcaa-c50d32925fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "x = pd.read_csv(dtapath + 'gdpquarterlychina1992.1-2017.4.csv',header=0)\n",
    "dates = pd.date_range(start='1992',periods=len(x),freq='QE')\n",
    "x.index=dates\n",
    "\n",
    "# Plot the time series\n",
    "x.plot()\n",
    "plt.title('Chinese Quarterly GDP 1992-2017')\n",
    "plt.ylabel('billions of RMB')\n",
    "plt.show()\n",
    "\n",
    "# Plot ACF\n",
    "plot_acf(x, lags=60)\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Lag Plot\n",
    "lag_plot(x)\n",
    "plt.title('Lag Plot of the Time Series')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0fb5b-c8a7-44c7-9837-3c892b1d57f3",
   "metadata": {},
   "source": [
    "### **Example 2: Quarterly Exchange Rates of GBP to NZ Dollar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e795b9-0f7e-43dc-b207-4fd707894d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the exchange rate data\n",
    "x = pd.read_csv(dtapath + 'ExchRate NZ per UK.txt', header=0)\n",
    "\n",
    "# Create a date range starting from 1991 with quarterly frequency\n",
    "dates = pd.date_range('1991', periods=len(x), freq='QE')\n",
    "\n",
    "# Set the index to the created date range\n",
    "x.index = dates\n",
    "\n",
    "# Create a time series from the 'xrate' column\n",
    "xts = pd.Series(x['xrate'])\n",
    "\n",
    "# Plot the time series\n",
    "xts.plot()\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Exchange rate')\n",
    "plt.title('Exchange Rate NZ per UK (1991 onward)')\n",
    "plt.show()\n",
    "\n",
    "# Plot ACF\n",
    "fig = plt.figure()\n",
    "plot_acf(xts, lags=17)\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Plot PACF\n",
    "fig = plt.figure()\n",
    "plot_pacf(xts, lags=17)\n",
    "plt.title(\"PACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Lag Plot\n",
    "lag_plot(xts)\n",
    "plt.title('Lag Plot of the Time Series')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8beedb-1cdc-4840-8a06-5c47281f5992",
   "metadata": {},
   "source": [
    "## 3. Random walks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a876b5-1cdc-4e44-adf4-8249480ceab0",
   "metadata": {},
   "source": [
    "Three simulated paths (time plots) of the standard normal random walk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324876a-2f94-4b8d-baf1-f14722145577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3debf9-34f2-40ca-b053-b593a646bb66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1357)\n",
    "a=normal(size=300); b=normal(size=300); c=normal(size=300)\n",
    "x=np.cumsum(a); y=np.cumsum(b); z=np.cumsum(c)\n",
    "xyz=pd.DataFrame({'x': x, 'y': y, 'z': z})\n",
    "xyz.index=range(1,301)\n",
    "xyz.plot(style=['-', '--', ':']); plt.show() # style means matplotlib line style per column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f78a7-f441-4d92-af02-c8acf13b074f",
   "metadata": {},
   "source": [
    "## 4. White noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4c913-10f0-47ae-b9fc-2b284f4b47b7",
   "metadata": {},
   "source": [
    "## 4.1 Simulating a Gaussian white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd4c7b-4c5a-4370-9fa9-2860c04156b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50878bef-8059-495d-b3ee-22e5b11e4a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(135) # for repeat\n",
    "x=random.normal(loc=0, scale=1, size=1000)\n",
    "xts=pd.Series(x)\n",
    "xts.plot(); plt.xlabel('Time')\n",
    "plt.ylabel('Simulated white noise')\n",
    "plt.show()\n",
    "\n",
    "plot_acf(xts, lags=30) # plotting ACF\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c2918-0dcc-4642-803a-5e2ad4fa72e3",
   "metadata": {},
   "source": [
    "It is an intuitive method for testing that a stationary time series is a white noise or not: To examine its ACF plot and if the ACF plot is similar to this figure, then we are apt to think that the time series is a white noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19be1f-aeca-43d2-a4e6-5c33f636fe99",
   "metadata": {},
   "source": [
    "## 4.2 Chaos like a white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfeddad-92ae-42fd-966f-9954193c7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty pandas Series with a float data type\n",
    "x = pd.Series(dtype=float)\n",
    "\n",
    "# Start value for y\n",
    "y = 0.3\n",
    "\n",
    "# Use a for loop to generate the values\n",
    "for t in range(1, 501):\n",
    "    y = 4.0 * y * (1 - y)  # Logistic map equation\n",
    "    x.loc[t-1] = y  # Assigning the value at index t-1\n",
    "\n",
    "# Set the index of the series to be in the range 1 to 500\n",
    "x.index = range(1, 501)\n",
    "\n",
    "# Display the result\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d9b2d1-bc96-4b06-a834-2459a9be8a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.plot()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Simulated Chaos')\n",
    "plt.show()\n",
    "\n",
    "plot_acf(x, lags=30) # plotting ACF\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6246427-acbd-4079-938c-4335d6c3298d",
   "metadata": {},
   "source": [
    "## 4.3 White noise test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e7c82-7bf4-47c3-9e36-c7a6f3e687ed",
   "metadata": {},
   "source": [
    "How to statistically test whether a stationary time series is a white noise?\n",
    "\n",
    "For a stationary time series $\\{X_t\\}$ is a white noise if and only if its autocorrelation function $(ACF) \\rho_k = 0$ for any integer $k = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a31743e-5eed-4fe1-ba0d-d828cf62c665",
   "metadata": {},
   "source": [
    "### **Example 2 [continues]: Quarterly Exchange Rates of GBP to NZ Dollar**\n",
    "\n",
    "The quarterly exchange rates of GBP to NZD are a financial time series. We often analyze their logarithms because this stabilizes the average and variability, reduces trends, and makes the data more normal. This is important due to the volatility in financial data. Logging also highlights relative changes instead of absolute values, which is key for understanding percentage returns and growth rates in finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a898b2a-b088-4213-a009-53ad74d103bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithm of the series:\n",
    "logxts = np.log(xts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1be2649-b969-405d-8715-f6788ed2f3cf",
   "metadata": {},
   "source": [
    "We need to difference the time series to achieve stationarity before testing for white noise. Differencing will be more closely investigated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a8b7b1-90f2-4ad5-8f7a-91746b46b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogxts = logxts.diff(1).dropna()  # Remove NaN values resulting from differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb2f24-b4b5-4027-a091-b7c53f3ac15c",
   "metadata": {},
   "source": [
    "Test for white noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c73eb32-b820-4fa3-9736-c2c144563dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the differenced logarithmic series\n",
    "dlogxts.plot(marker='o', markersize=5)\n",
    "plt.title('Difference of Logarithm of the ExchRate NZ per UK')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Difference of Log Price')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the ACF\n",
    "plot_acf(dlogxts, lags=17)\n",
    "plt.title(\"ACF of the Time Series\")\n",
    "plt.show()\n",
    "\n",
    "# Calculating ACF, Ljung-Box statistics, and p-values\n",
    "r, q, p = acf(dlogxts, nlags=35, qstat=True)\n",
    "print('ACF values:', r) \n",
    "print('Ljung-Box statistics:', q)\n",
    "print('P-values:', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c06c450-35b0-4ffc-a11d-caaa57e87d6c",
   "metadata": {},
   "source": [
    "The Ljung-Box statistic checks if there are significant autocorrelations at multiple lags. The null hypothesis is that the data are independently distributed (i.e., white noise). If the p-value is low (commonly less than 0.05), we reject the null hypothesis.\n",
    "\n",
    "The differentiated time series, indicated by the p-values from the Ljung-Box test, suggests that it behaves like white noise at higher lags since the p-values for those lags are greater than 0.05, indicating no significant autocorrelation. However, since the earlier p-values (for the first few lags) indicate some level of autocorrelation, we can't definitively say that the entire differentiated time series is purely white noise. It is approaching white noise behavior but might still have some dependencies at the initial lags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725f2aa-0d4f-4538-8ce6-b8425ea9f67a",
   "metadata": {},
   "source": [
    "## 5. Time Series Features: Time Series Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f57f7-c4f6-4bee-946f-7cb4bb9a2f7a",
   "metadata": {},
   "source": [
    " In practice, many of realistic time series possess either deterministic seasonality (component) or a deterministic trend (component). Some of them may have both. After extracting the trend and seasonal components from a time series, the remainder is its random (variation) component. Decomposing a time series is helpful to better understand it and improve forecast accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c0bc3-d695-41d8-822a-6fd76864f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import month_plot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524d5d3-8466-4afd-8a66-e80ed7b7b2a4",
   "metadata": {},
   "source": [
    "### **Example 3: Australian Employed Total Persons**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4f79f-00f7-4c4e-8aa3-49da1a21c6f2",
   "metadata": {},
   "source": [
    "The \"Australia Employed Total Persons\" dataset tracks the monthly total number of employed people in Australia from February 1978 to November 2018. Initially, the time series shows a steady upward trend with no apparent seasonality. However, when zooming in on data from January 2013 to January 2017, clear seasonal patterns emerge, repeating every year. Seasonal plots confirm that these patterns are consistent with little variation over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b5f2f-3c99-4237-9a11-608448ae24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data file path\n",
    "dtapath = getdtapath()\n",
    "\n",
    "# Load the Excel file containing employment data\n",
    "aul = pd.read_excel(dtapath + 'AustraliaEmployedTotalPersons.xlsx', header=0)\n",
    "\n",
    "# Create a time index starting from February 1978 with monthly frequency\n",
    "timeindex = pd.date_range('1978-02', periods=len(aul), freq='ME')\n",
    "aul.index = timeindex\n",
    "\n",
    "# Extract the 'EmployedP' column for analysis\n",
    "aults = aul['EmployedP']\n",
    "\n",
    "# Plot the time series\n",
    "aults.plot()\n",
    "plt.title('Total Employed Persons in Australia')\n",
    "plt.ylabel('Number of Persons Employed')\n",
    "plt.xlabel('Date')\n",
    "plt.show()\n",
    "\n",
    "# Graph time series plot from January 2013 to January 2017\n",
    "aults['2013-01':'2017-01'].plot()\n",
    "plt.title('Total Employed Persons (2013-01 to 2017-01)')\n",
    "plt.ylabel('Number of Persons Employed')\n",
    "plt.xlabel('Date')\n",
    "plt.show()\n",
    "\n",
    "# Plot seasonal plots\n",
    "month_plot(aults)\n",
    "plt.title('Monthly Employment Data Seasonal Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd70d3c-662d-4d17-8fb2-91fd8b9e7f82",
   "metadata": {},
   "source": [
    "Using an additive model to decompose the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e798f7-8b1f-4865-8ba7-90c8c8d99b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the time series using an additive model (Time series=Trend+Seasonality+Residual)\n",
    "aultsdeca = seasonal_decompose(aults, model='additive')\n",
    "aultsdeca.plot()\n",
    "plt.title('Seasonal Decomposition of Employment Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b083c-064f-426a-8fff-701fc7cca4ae",
   "metadata": {},
   "source": [
    "After decomposition, the residuals should ideally represent the random noise that remains after removing trend and seasonal patterns. By plotting the ACF of the residuals, we can determine if any patterns or correlations were not captured by the decomposition. A random noise pattern should show insignificant autocorrelation (i.e., ACF values near zero). \n",
    "In addition, rolling means help evaluate if the average of the residuals is constant, indicating stationarity. A trend in the rolling mean suggests a non-stationary component. Rolling standard deviations reveal the variability of the residuals; significant changes may indicate heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca002cb3-65ec-450f-a555-106c7ff9a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract residuals from the decomposition and drop NaN values\n",
    "aultsdecaResid = aultsdeca.resid.dropna()\n",
    "\n",
    "# Plot ACF of the residuals\n",
    "plot_acf(aultsdecaResid, lags=48)\n",
    "plt.title('ACF of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Rolling mean and standard deviation\n",
    "rolm = aultsdecaResid.rolling(window=36, center=True).mean()\n",
    "rolstd = aultsdecaResid.rolling(window=36, center=True).std()\n",
    "\n",
    "# Plotting residuals, rolling mean, and rolling std\n",
    "plt.plot(aultsdecaResid, label='Decomposed Residuals')\n",
    "plt.plot(rolm, label='Rolling Mean', linestyle='--')\n",
    "plt.plot(rolstd, label='Rolling Std', linestyle=':', c='red')\n",
    "plt.title('Residuals of Australian Employed Persons')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e27f9c-38b1-4b0f-aa3d-b5d3844917ee",
   "metadata": {},
   "source": [
    "The residuals are mostly stationary, though there are seasonal correlations at specific lags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24786d18-a438-4815-991b-a2704880fb72",
   "metadata": {},
   "source": [
    "## 6. Multivariate Time Series\n",
    "\n",
    "In multivariate time series analysis, we examine multiple interrelated variables simultaneously instead of just one. This analysis helps us understand how these variables influence each other over time and enables joint forecasting. The methods used for analyzing single time series can often be adapted for multivariate scenarios, but it becomes essential to consider how the interactions between these variables affect their individual behaviors. The following are some specific analyses for exploring the dynamic relationships and dependencies inherent in multivariate datasets and time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde5319-98e9-47d3-9f73-2a89d1f3dfa3",
   "metadata": {},
   "source": [
    "### **Example 4: Large Dataset of Synthetic Time Series**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace17745-374e-4bbb-85af-f046f9932edd",
   "metadata": {},
   "source": [
    "We generate multiple synthetic independent time series with trend, seasonal effect and random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcc054-48bc-404b-8a7e-0d43547de316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic time series data\n",
    "dates = pd.date_range('2000-01-01', periods=100, freq='M')\n",
    "\n",
    "# Create a DataFrame to hold the time series\n",
    "data = pd.DataFrame(index=dates)\n",
    "\n",
    "# Generate time series\n",
    "n_series = 24\n",
    "for i in range(1, n_series + 1):  # 24 time series\n",
    "    trend = np.linspace(0, np.random.uniform(0.1, 1), len(dates))  # Linear trend\n",
    "    seasonal = 0.1 * np.sin(np.linspace(0, 3 * np.pi, len(dates)))  # Seasonal effect\n",
    "    noise = np.random.normal(loc=0, scale=0.05, size=len(dates))  # Random noise\n",
    "    series = trend + seasonal + noise  # Combine to create the time series\n",
    "    data[f'Series_{i}'] = series\n",
    "\n",
    "# Plotting all the time series; highlight the last two series\n",
    "plt.figure()\n",
    "for column in data.columns:\n",
    "    plt.plot(data.index, data[column], label=column, color='blue')\n",
    "plt.title('Synthetic Time Series Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.grid()\n",
    "plt.tight_layout()  # Adjust the layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753d715-a074-4bbc-8329-945fb5d2dfb4",
   "metadata": {},
   "source": [
    "## 6.1 Pearson Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d5f58-ae45-455c-b09c-20f5776e4ecf",
   "metadata": {},
   "source": [
    "Pearson correlation measures how strong and in what direction two continuous variables are related. It ranges from -1 to 1: a value of 1 means a perfect positive relationship, -1 means a perfect negative relationship, and 0 means no relationship. It looks at how much one variable changes in relation to another and is commonly used to understand connections between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c7b5e-d479-4c18-9827-f6d321a34f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Pearson correaltion\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# plot heatmap\n",
    "import seaborn as sns # another library for plotting\n",
    "plt.figure()\n",
    "sns.heatmap(correlation_matrix, annot=False, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Pearson Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Find the two most correlated series\n",
    "corr_values = correlation_matrix.unstack().sort_values(ascending=False)\n",
    "corr_values = corr_values[corr_values < 1.0] # Exclude self-correlations (1.0) \n",
    "most_correlated_pair = corr_values.idxmax()\n",
    "series1, series2 = most_correlated_pair\n",
    "print(f'Most Correlated Series: {series1} and {series2}')\n",
    "\n",
    "# Plotting all the time series, highlighting the two most correlated series\n",
    "plt.figure()\n",
    "for column in data.columns:\n",
    "    plt.plot(data.index, data[column], label=column, color='lightgrey')\n",
    "plt.plot(data.index, data[series1], label=series1, color='blue', linewidth=2.5)  # First highlighted series\n",
    "plt.plot(data.index, data[series2], label=series2, color='blue', linewidth=2.5)  # Second highlighted series\n",
    "plt.title('Synthetic Time Series Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1d370-45e8-4dcf-a2ec-6bdccfa0ea46",
   "metadata": {},
   "source": [
    "## 6.2 Cross-Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d66e5-efb0-4aea-8db0-d25ef65752ff",
   "metadata": {},
   "source": [
    "The cross-correlation between the two selected time series measures how similar they are at different time lags. We limit the analysis to a maximum of 15 lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f397361a-6129-4080-942c-01fc34fda853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import ccf\n",
    "\n",
    "max_lag = 15\n",
    "cross_corr = ccf(data[series1], data[series2])[:max_lag]\n",
    "\n",
    "# Plot Cross-Correlation\n",
    "plt.figure()\n",
    "plt.stem(range(max_lag), cross_corr)\n",
    "plt.title(f\"Cross-Correlation between {series1} and {series2}\")\n",
    "plt.xlabel(\"Lags\")\n",
    "plt.ylabel(\"Cross-Correlation\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c67b0-211d-4c5f-a880-53f7a7ed7665",
   "metadata": {},
   "source": [
    "The plot reveals a strong correlation at lag 0, indicating that the two series change together simultaneously. As we increase the lag, the correlation slowly decreases but remains relatively high up to 14 time units, showing a positive relationship that weakens over time. In summary, the plot demonstrates that Series_4 and Series_3 are closely connected, and this relationship continues even when one series is shifted over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f233f-4d81-4475-8ab2-e2c91a0be4a2",
   "metadata": {},
   "source": [
    "## 6.3 Dimensionality Reduction: Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb97c2-cc83-480a-a505-be97d1df98d2",
   "metadata": {},
   "source": [
    "We create a PCA model and specify that we want to keep 5 principal components (PCs). We then transform the standardized data (discussed later in more detail) to extract these components, which capture the most important patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb93b8-de7f-4e6b-af78-058668cf9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "# calculate PCs\n",
    "pca = PCA(n_components=5)  # Adjust number of components if necessary\n",
    "pca_scores = pca.fit_transform(standardized_data)\n",
    "pca_df = pd.DataFrame(data=pca_scores, columns=[f'PC_{i + 1}' for i in range(pca.n_components_)], index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f3b3a-f261-4dcf-980b-fd2db66f2693",
   "metadata": {},
   "source": [
    "The explained variance in the PCA describes how much variance (or information) each PC captures. Higher values mean the component is more important. It helps us understand how many components we might need to keep for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04936df5-f072-4265-bc81-f90fdfd9c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the explained variance\n",
    "plt.figure()\n",
    "plt.plot(range(1, 6), pca.explained_variance_ratio_, marker='o')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1992610-39d7-493c-a6b2-17081fc11d63",
   "metadata": {},
   "source": [
    "We visualize the values of the first two PCs over time. This helps us see how the main patterns in the data change, making it easier to explore trends and relationships in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7e291-b550-4d24-84d4-a86222090e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the first two principal components\n",
    "\n",
    "# time series plot\n",
    "plt.figure()\n",
    "plt.plot(pca_df.index, pca_df['PC_1'], label='Principal Component 1', color='blue')\n",
    "plt.plot(pca_df.index, pca_df['PC_2'], label='Principal Component 2', color='blue', linestyle='--')\n",
    "plt.title('First Two Principal Components')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()  # Adjust the layout\n",
    "plt.show()\n",
    "\n",
    "# scatterplot\n",
    "plt.figure()\n",
    "sns.scatterplot(data=pca_df, x='PC_1', y='PC_2', color='blue')\n",
    "plt.title('First Two Principal Components')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
